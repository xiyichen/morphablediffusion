<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    
    <link rel="icon" type="image/png" href="./files/preview_icon.jpg"> 
    <title>Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="./files/figure1.png">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://xiyichen.github.io/morphablediffusion/">
    <meta property="og:title" content="Morphable Diffusion">
    <meta property="og:description" content="3D-Consistent Diffusion for Single-image Avatar Creation">

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Morphable Diffusion">
    <meta name="twitter:description" content="Project page for Morphable Diffusion.">
    <meta name="twitter:image" content="https://xiyichen.github.io/morphablediffusion/files/teaser.png">


    <link rel="icon" type="image/png" href="./files/favicon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="./files/bootstrap.min.css">
    <link rel="stylesheet" href="./files/font-awesome.min.css">
    <link rel="stylesheet" href="./files/codemirror.min.css">
    <link rel="stylesheet" href="./files/app.css">

    <link rel="stylesheet" href="./files/bootstrap.min(1).css">
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1S6DD7H8WE"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-1S6DD7H8WE');
    </script>


    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

    <script src="./files/jquery.min.js"></script>
    <script src="./files/bootstrap.min.js"></script>
    <script src="./files/codemirror.min.js"></script>
    <script src="./files/clipboard.min.js"></script>
    
    <script src="./files/app.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.1101.0" data-gr-ext-installed="">
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Morphable Diffusion:</b>  <br> 
                <!-- Towards Efficient and Scalable Dynamic Surface Representations<br> -->
                <small> 3D-Consistent Diffusion for Single-image Avatar Creation <br>  arXiv 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://xiyichen.github.io">
                          Xiyi Chen
                        </a><sup>1</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                    <li>
                        <a href="https://markomih.github.io">
                          Marko Mihajlovic
                        </a><sup>1</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                    <li>
                        <a href="https://taconite.github.io">
                          Shaofei Wang
                        </a><sup>1</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                    <li>
                        <a href="https://vlg.inf.ethz.ch/team/Dr-Sergey-Prokudin.html">
                          Sergey Prokudin
                        </a><sup>1</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                    <li>
                        <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">
                          Siyu Tang
                        </a><sup>1</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
            <span class="author-block"><sup>1</sup>ETH Zürich <br></span>&nbsp;&nbsp;
            <!-- <span class="author-block"><sup>2</sup>Max Planck Institute for Intelligent Systems, Tübingen <br></span>&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>Microsoft</span> -->
          </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="./#">
                            <img src="./files/paper.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="./#">
                            <img src="./files/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong><br></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/xiyichen/morphablediffusion">
                            <img src="./files/github.png" height="60px">
                                <h4><strong>Code (coming soon!)</strong><br></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <div class="video-container">
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/teaser_2.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
            </div>
            <p class="text-justify">
                <b>TL;DR:</b> We introduce a morphable diffusion model to enable consistent controllable novel view synthesis of humans from a single image. 
        Given a single input image and a morphable mesh with a desired facial expression, our method directly generates 3D consistent and photo-realistic images from novel viewpoints, which we could use to reconstruct a coarse 3D model using off-the-shelf neural surface reconstruction methods such as <a href="https://vcai.mpi-inf.mpg.de/projects/NeuS2/">NeuS2</a>.
            </p>
            </div>
            
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview video
                </h3>
                Coming Soon!
            </div>
        </div>
<!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    The model
                </h3>
                
                <p class="text-justify">
                    Our model consists of an optimisable canonical point set (left) and a compact neural network that maps every point and its associated feature in a canonical cloud to a new location.
                </p>
            </div>
        </div>
-->
    

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Pipeline
                </h3>
            <img src="./files/architecture.jpg" class="img-responsive" alt="overview">
            <br>
            <p class="text-justify">
                <b>Our pipeline consists of 4 parts:</b>
                <br>

                <b>A)</b> a noise feature volume is constructed by lifting and processing the 2D noise features into a 3D volume $\mathbf{G}$ which is processed by a 3D CNN $\eta_\theta$ to produce a feature grid $\mathbf{F}_G$. 
                <br>
                <b>B)</b> a morphable noise volume is constructed by attaching the 2D noise features onto mesh vertices that are processed by a SparseConvNet $f_\theta$ to output a 3DMM-aware feature volume $\mathbf{F}_V$. 
                <br>
                <b>C)</b> the resulting feature grids $\mathbf{F}_G$ and $\mathbf{F}_V$ are blended via addition. The blended noise volume is further interpolated to the frustum $\mathbf{F}^{(i)}$ of a target view $(i)$ that we wish to synthesize. 
                <br>
                <b>D)</b> the noisy target image $\mathbf{x}_t^{(i)}$, the input image $\mathbf{y}$, and the target feature frustum are processed by a pre-trained 2D UNet akin to <a href="https://liuyuan-pal.github.io/SyncDreamer/">SyncDreamer</a> to predict the denoised image at the next iteration $\mathbf{x}_{t-1}^{(i)}$.
            </p>
            </div>
            
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Novel View Synthesis of Faces
                </h3>
                <p class="text-justify">
                    We test our method for single-view reconstruction of human faces on the <a href="https://facescape.nju.edu.cn/">FaceScape</a> dataset and compare our method with <a href="https://nvlabs.github.io/eg3d/">EG3D</a>, <a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>, <a href="https://lakonik.github.io/ssdnerf/">SSDNeRF</a>, and <a href="https://liuyuan-pal.github.io/SyncDreamer/">SyncDreamer</a>. For EG3D, we use <a href="https://github.com/oneThousand1000/EG3D-projector">a custom GAN inversion repository</a> to search for the latent code that produces the best resemblance. Our method produces the best scores on the perceptual metrics while preserving accurate facial expressions and good resemblance, which is attributed to the effective conditioning on the morphable model.
                </p>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nvs_1.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nvs_2.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                
            </div>
        </div>

        
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Novel Facial Expression Synthesis
                </h3>
                <p class="text-justify">
                    We test our method for novel facial expression synthesis from single image and compare our method with <a href="https://yiyuzhuang.github.io/mofanerf/">MoFaNeRF</a>. Our method produces better resemblance.
                </p>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nes_1.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nes_2.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                <p class="text-justify">
                    We finetune our method on 16 views of the test subject in neutral expression and compare the finetuned model with <a href="https://diffusionrig.github.io/">DiffusionRig</a>. Our finetuned model improves the resemblance slightly compared to the results with single input image, and produces images with better fidelity compared to the baseline.
                </p>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nes_1_finetuned.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nes_2_finetuned.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Novel View Synthesis of Full-bodies
                </h3>
                <p class="text-justify">
                We also test our method for single-view reconstruction of human full-body images on the <a href="https://github.com/ytrock/THuman2.0-Dataset">Thuman2.0</a> dataset and compare our method with <a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>, <a href="https://zero123.cs.columbia.edu/">zero-1-to-3</a>, and <a href="https://liuyuan-pal.github.io/SyncDreamer/">SyncDreamer</a>. Given a single full-body image in any pose, our method produces novel views of the person with the most accurate poses.
                </p>
                <img src="./files/nvs_full_body.jpg" class="img-responsive" alt="overview">
                
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Full-body animation
                </h3>
                <p class="text-justify">
                    We also show that our method could be applied to animate a full-body image using SMPL meshes as driving signals.
                </p>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/full_body_animation.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                
            </div>
        </div>
        
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <!-- <div class="form-group col-md-10 col-md-offset-1">
                <textarea id="bibtex" class="form-control" readonly>
                @article{prokudin2023dynamic,
                  title={Dynamic Point Fields},
                  author={Prokudin, Sergey and Ma, Qianli and Raafat, Maxime and Valentin, Julien and Tang, Siyu},
                  journal={arXiv preprint arXiv:2304.02626},
                  year={2023}
                }</textarea>
                </div> -->
                Coming Soon!
            </div>
        </div>
        
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank Korrawe Karunratanakul for discussion about per-subject finetuning of diffusion models, Timo Bolkart for his advice on fitting FLAME models, and Malte Prinzler for his help with the color-calibrated FaceScape data. Marko Mihajlovic and Sergey Prokudin are supported by the Innosuisse Flagship project PROFICIENCY No. PFFS-21-19. Shaofei Wang acknowledges the SNF grant 200021 204840.
                <br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>


</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>