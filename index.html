<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    
    <link rel="icon" type="image/png" href="./files/preview_icon.jpg"> 
    <title>Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="./files/figure1.png">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://xiyichen.github.io/morphablediffusion/">
    <meta property="og:title" content="Morphable Diffusion">
    <meta property="og:description" content="3D-Consistent Diffusion for Single-image Avatar Creation">

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Morphable Diffusion">
    <meta name="twitter:description" content="Project page for Morphable Diffusion.">
    <meta name="twitter:image" content="https://xiyichen.github.io/morphablediffusion/files/teaser.png">


    <link rel="icon" type="image/png" href="./files/favicon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="./files/bootstrap.min.css">
    <link rel="stylesheet" href="./files/font-awesome.min.css">
    <link rel="stylesheet" href="./files/codemirror.min.css">
    <link rel="stylesheet" href="./files/app.css">

    <link rel="stylesheet" href="./files/bootstrap.min(1).css">
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1S6DD7H8WE"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-1S6DD7H8WE');
    </script>


    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

    <script src="./files/jquery.min.js"></script>
    <script src="./files/bootstrap.min.js"></script>
    <script src="./files/codemirror.min.js"></script>
    <script src="./files/clipboard.min.js"></script>
    
    <script src="./files/app.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.1101.0" data-gr-ext-installed="">
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Morphable Diffusion:</b>  <br> 
                <!-- Towards Efficient and Scalable Dynamic Surface Representations<br> -->
                <small> 3D-Consistent Diffusion for Single-image Avatar Creation <br>  CVPR 2024
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://xiyichen.github.io">
                          Xiyi Chen
                        </a><sup>1</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                    <li>
                        <a href="https://markomih.github.io">
                          Marko Mihajlovic
                        </a><sup>1</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                    <li>
                        <a href="https://taconite.github.io">
                          Shaofei Wang
                        </a><sup>1,2,3</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=xSywCzAAAAAJ&hl=en">
                          Sergey Prokudin
                        </a><sup>1,4</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                    <li>
                        <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">
                          Siyu Tang
                        </a><sup>1</sup>
<!--                        <br>ETH Zürich-->
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
            <span class="author-block"><sup>1</sup><a href="https://ethz.ch/en.html">ETH Zürich</a><br></span>&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup><a href="https://uni-tuebingen.de/en/">University of Tübingen</a><br></span>&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup><a href="https://tuebingen.ai/">Tübingen AI Center</a><br></span>&nbsp;&nbsp;
            <span class="author-block"><sup>4</sup><a href="https://rocs.balgrist.ch/en/">ROCS, University Hospital Balgrist, University of Zürich</a></span>
          </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2401.04728">
                            <img src="./files/paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=keF7qHeyxJc">
                            <img src="./files/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong><br></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/xiyichen/morphablediffusion">
                            <img src="./files/github.png" height="60px">
                                <h4><strong>Code</strong><br></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <div class="video-container">
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/teaser_2.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
            </div>
            <p class="text-justify">
                <b>TL;DR:</b> We introduce a morphable diffusion model to enable consistent controllable novel view synthesis of humans from a single image. 
        Given a single input image and a morphable mesh with a desired facial expression, our method directly generates 3D consistent and photo-realistic images from novel viewpoints, which we could use to reconstruct a coarse 3D model using off-the-shelf neural surface reconstruction methods such as <a href="https://vcai.mpi-inf.mpg.de/projects/NeuS2/">NeuS2</a>.
            </p>
            </div>
            
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview Video
                </h3>
                <!-- <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/keF7qHeyxJc" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div> -->
                Coming Soon!
            </div>
        </div>
<!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    The model
                </h3>
                
                <p class="text-justify">
                    Our model consists of an optimisable canonical point set (left) and a compact neural network that maps every point and its associated feature in a canonical cloud to a new location.
                </p>
            </div>
        </div>
-->
    

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Pipeline
                </h3>
            <img src="./files/architecture.jpg" class="img-responsive" alt="overview">
            <br>
            <p class="text-justify">
                <b>Our pipeline consists of 4 parts:</b>
                <br>

                <b>A)</b> a noise feature volume is constructed by lifting and processing the 2D noise features into a 3D volume $\mathbf{G}$ which is processed by a 3D CNN $\eta_\theta$ to produce a feature grid $\mathbf{F}_G$. 
                <br>
                <b>B)</b> a morphable noise volume is constructed by attaching the 2D noise features onto mesh vertices that are processed by a SparseConvNet $f_\theta$ to output a 3DMM-aware feature volume $\mathbf{F}_V$. 
                <br>
                <b>C)</b> the resulting feature grids $\mathbf{F}_G$ and $\mathbf{F}_V$ are blended via addition. The blended noise volume is further interpolated to the frustum $\mathbf{F}^{(i)}$ of a target view $(i)$ that we wish to synthesize. 
                <br>
                <b>D)</b> the noisy target image $\mathbf{x}_t^{(i)}$, the input image $\mathbf{y}$, and the target feature frustum are processed by a pre-trained 2D UNet akin to <a href="https://liuyuan-pal.github.io/SyncDreamer/">SyncDreamer</a> to predict the denoised image at the next iteration $\mathbf{x}_{t-1}^{(i)}$.
            </p>
            </div>
            
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Novel View Synthesis of Faces
                </h3>
                <p class="text-justify">
                    We test our method for single-view reconstruction of human faces on the <a href="https://facescape.nju.edu.cn/">FaceScape</a> dataset and compare our method with <a href="https://nvlabs.github.io/eg3d/">EG3D</a>, <a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>, <a href="https://lakonik.github.io/ssdnerf/">SSDNeRF</a>, and <a href="https://liuyuan-pal.github.io/SyncDreamer/">SyncDreamer</a>. For EG3D, we use <a href="https://github.com/oneThousand1000/EG3D-projector">a custom GAN inversion repository</a> to search for the latent code that produces the best resemblance. Our method produces the best scores on the perceptual metrics while preserving accurate facial expressions and good resemblance, which is attributed to the effective conditioning on the morphable model.
                </p>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nvs_1.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nvs_2.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                
            </div>
        </div>

        
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Novel Facial Expression Synthesis
                </h3>
                <p class="text-justify">
                    We test our method for novel facial expression synthesis from single image and compare our method with <a href="https://yiyuzhuang.github.io/mofanerf/">MoFaNeRF</a>. Our method produces better resemblance.
                </p>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nes_1.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nes_2.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                <p class="text-justify">
                    We finetune our method on 16 views of the test subject in neutral expression and compare the finetuned model with <a href="https://diffusionrig.github.io/">DiffusionRig</a>. Our finetuned model improves the resemblance slightly compared to the results with single input image, and produces images with better fidelity compared to the baseline.
                </p>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nes_1_finetuned.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/nes_2_finetuned.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Novel Facial Expression Synthesis on in-the-wild Face Images
                </h3>


                <p class="text-justify">
                    We test our method on in-the-wild face images generated by <a href="https://github.com/NVlabs/stylegan2">StyleGAN2</a>. You can try our interactive viewer with individual control of view/identity/facial expression.
                </p>

                <div class="slidercontainer">
                    <div class="slider-container">
                        <div>
                            <img id="identityImage" src="./stylegan_ids/stylegan1.png" alt="Identity Image" class="identity-image">
                            <input type="range" min="1" max="5" value="1" class="slider" id="identitySlider">
                            <label for="identitySlider">Identity: <span id="identityName">stylegan1</span></label>
                        </div>
                        <div>
                            <img id="expressionImage" src="./demo_exps/kiss.png" alt="Expression Image" class="expression-image">
                            <input type="range" min="0" max="4" value="0" class="slider" id="expressionSlider">
                            <label for="expressionSlider">Expression: <span id="expressionName">kiss</span></label>
                        </div>
                        <div>
                            <input type="range" min="0" max="15" value="8" class="slider" id="imageSlider">
                            <label for="imageSlider">View</label>
                        </div>
                    </div>
                    <img id="displayImage" src="./results/1/kiss/7.png" alt="Image Display">
                </div>
                <script src="./files/slider.js"></script>
                
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Novel View Synthesis of Full Bodies
                </h3>
                <p class="text-justify">
                We also test our method for single-view reconstruction of human full-body images on the <a href="https://github.com/ytrock/THuman2.0-Dataset">Thuman2.0</a> dataset and compare our method with <a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>, <a href="https://zero123.cs.columbia.edu/">zero-1-to-3</a>, and <a href="https://liuyuan-pal.github.io/SyncDreamer/">SyncDreamer</a>. Given a single full-body image in any pose, our method produces novel views of the person with the most accurate poses.
                </p>
                <img src="./files/nvs_full_body.jpg" class="img-responsive" alt="overview">
                
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Full-body Animation
                </h3>
                <p class="text-justify">
                    We also show that our method could be applied to animate a full-body image using SMPL meshes as driving signals.
                </p>
                <video width="auto" height="100" playsinline loop autoplay muted>
                  <source src="./files/full_body_animation.mp4" type="video/mp4">
                    <style>
                        video {
                            height: 100%;
                            width: 100%;
                            object-fit: cover; // use "cover" to avoid distortion
                            position: absolute;
                        }
                    </style>
                </video>
                
            </div>
        </div>
        
        
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                <textarea id="bibtex" class="form-control" readonly>
                @misc{chen2024morphable,
                      title={Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation}, 
                      author={Xiyi Chen and Marko Mihajlovic and Shaofei Wang and Sergey Prokudin and Siyu Tang},
                      year={2024},
                      eprint={2401.04728},
                      archivePrefix={arXiv},
                      primaryClass={cs.CV}
                }</textarea>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h3>
                    Citation
            </h3>
            <div class="card">
                <div class="card-body">
                <!-- <pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px"> -->
                <pre>@article{chen2024morphable,
      title={Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation}, 
      author={Xiyi Chen and Marko Mihajlovic and Shaofei Wang and Sergey Prokudin and Siyu Tang},
      booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
      year={2024}
      }<font></font>
                </pre>
                </div>
            </div>
            </div>
        </div>
        
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank Korrawe Karunratanakul for the discussion on per-subject finetuning of diffusion models, Timo Bolkart for the advice on fitting FLAME model, and Malte Prinzler for the help with the color-calibrated FaceScape data. This project is partially supported by the SNSF grant 200021 204840. Shaofei Wang also acknowledges support from the ERC Starting Grant LEGO-3D (850533) and the DFG EXC number 2064/1 - project number 390727645.
                <br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>


</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>